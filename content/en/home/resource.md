# Resources

2025-05-28 [\[Paper\] Large language models often know when they are being evaluated](https://arxiv.org/abs/2505.23836)

Joe Needham, Giles Edkins, Govind Pimpale, Henning Bartsch, Marius Hobbhahn

--

2025-05-24 [\[Post\] It's hard to make scheming evals look realistic for LLMs](https://www.lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms)
    
Igor Ivanov, Danil Kadochnikov

--

2025-03-23 [\[Paper\] Evaluation Faking: Unveiling Observer Effects in Safety Evaluation of Frontier AI Systems](https://arxiv.org/abs/2505.17815)

Yihe Fan, Wenqi Zhang, Xudong Pan, Min Yang

--

2025-05-20 [\[Paper\] Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models](https://arxiv.org/abs/2505.14617)

Sahar Abdelnabi, Ahmed Salem (Microsoft)

--

2025-05-03 [\[Post\] Exploring out-of-context reasoning (OOCR) fine-tuning in LLMs to increase test-phase awareness](https://www.lesswrong.com/posts/bhRYGzNGY3RxN3dNj/exploring-out-of-context-reasoning-oocr-fine-tuning-in-llms)

Sanyu Rajakumar

--

2025-04-15 [\[Post\] Can SAE steering reveal sandbagging?](https://www.lesswrong.com/posts/dBckLjYfTShBGZ8ma/can-sae-steering-reveal-sandbagging)

Jord Nguyen, Khiem Hoang, Carlo Leonardo Attubato, Felix Hofstätter

--

2025-03-17 [\[Post\] Claude Sonnet 3.7 (often) knows when it’s in alignment evaluations](https://www.apolloresearch.ai/blog/claude-sonnet-37-often-knows-when-its-in-alignment-evaluations)

Apollo Research

--

2025-03-17 [\[Document\] Probing and Steering Evaluation Awareness of Language Models](https://docs.google.com/document/d/1SEgV-resU_MQcjMiGy5Hge0vqshz165YtZU2BdV4ktI/edit?tab=t.0#heading=h.q9u27tyzz8i9)

Jord Nguyen, Khiem Hoang, Carlo Leonardo Attubato, Felix Hofstätter

--

2025-03-12 [\[Post\] Revising Stages-Oversight Reveals Greater Situational Awareness in LLMs](https://www.alignmentforum.org/posts/5naJwQnbb5bwPCCFz/revising-stages-oversight)

Sanyu Rajakumar

--

2024-07-05 [\[Paper\] Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs](https://arxiv.org/abs/2407.04694)

Rudolf Laine, Bilal Chughtai, Jan Betley, Kaivalya Hariharan, Jeremy Scheurer, Mikita Balesni, Marius Hobbhahn, Alexander Meinke, Owain Evans

--

